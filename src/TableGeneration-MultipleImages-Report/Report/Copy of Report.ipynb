{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Report.ipynb","provenance":[],"authorship_tag":"ABX9TyOUrCTVbMAJ0cLOm/uAewO0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5m5ZC6K5Rsmu","colab_type":"text"},"source":["# Reproducing Deep Image Prior Inpainting Results\n","## 1.Introduction\n","The goal of this project is to reproduce and evaluate results regarding the inpainting task reported in paper \"Deep Image Prior\" by Ulyanov et al. In this notebook, we will replicate the inpainting experiment done by the researchers while exploring new hyperparameter settings and architecture varieties to provide further insight on how these modifications may have impact on the output.\n","\n","The report is structured as the following. In Section 2, a brief definition of single image restoration will be provided followed by a summary of the proposed method for single image restoration by the researchers. \n","\n","In Section 3, Methods that are adopted for reproduction as well as their objectives will be explained. These are mainly ... ... ...\n","\n","In Section 4, the default architecture of the deep convolutional network used for the inpainting task will be introduced.\n","\n","The following sections of 5, 6, 7 include the experiments 1,2,3 which involve the objectives of .... ... ... respectively. In each of these sections, experimental setup will be introduced and related code will be executed to obtain results. \n","\n","In Section 8, results obtained from the experiments will be discussed in light of the explanations from the paper and our own background.\n","\n","In Section 9, the overal success of reproduction will be reported and findings will be addressed. \n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"c8epADqDR6j4","colab_type":"text"},"source":["## 2.Single Image Image Restoration Using Deep Image Priors\n","\n","### The Deep Image Restoration Paradigm\n","In current deep learning paradigm, the task of image restoration through generation of the restored image is commonly thought to involve training of the network parameters over a large dataset. A simple approach within this paradigm is to realize a network as below\n","\n","$$x = f_{\\theta}(x_0)$$\n","\n","where network parameters, clean image output and corrupted image input are denoted with $\\theta$,$x$,$x_0$ respectively. After training with multiple clean-corrupted image pairs, this network may be expected to output clean image $x$ when the corrupted image $x_0$ is presented as input in the test phase. \n","\n","The learned parameters achieve as a result of training on multiple images is expected to learn inverting the corruption by being able to extract image priors from the corrupted data. Formally, this network at hand should estimate the following likelihood;\n","\n","$$p(x|x_0) =^{\\~} p(x_0|x)p(x)$$\n","\n","Where $p(x)$ here refers to the image prior. In this particular image restoration task, it refers to the probability of pixel values formed by the actual content of the image before the corruption with its features such as edges, surfaces or higher level content. Consequently, a realistic estimate of the prior is needed for a sucdesful image restoration.\n","\n","The researchers are crticial of this approach, particularly due to the unclear nature of how priors are learned through this type of training and possiblity of overfit to the training samples instead. They claim that the information needed in order to capture an estimate of the image prior at least which includes a sufficient level of low-level image statistics can be extracted from the single \"test image\". Thus, the network can be handcrafted to contain prior information in its parameters without any training. The parameters can then be used for outputting a restored image estimate. \n","\n","### Realistic Image Generation using Encoder/Decoder Networks\n","\n","In order to implement their proposed method, the researchers utilize a generator network architecture which is used for outputting realistic images from random noise such that;\n","\n","$$a = f_{\\theta}(z)$$\n","\n","Where $z$ is an image formed with random noise and $a$ is the output image. The parameters of the network $f_{\\theta}$ determines nature of the content that will be generated. \n","\n","### Inverting Images Without Learning Through Energy Minimization\n","\n","Another method the researchers use is through minimization of loss without using learning. Let $\\alpha$, $x_0$, $\\alpha_*$ denote the variable image to be optimized, the corrupted image at hand and the solution for optimization problem respectively;\n","\n","$$ \\alpha_* = argmin_{\\alpha} L(\\alpha,x_0) $$\n","\n","However, this minimization will lead to a trivial solution where $\\alpha_*=x_0$. Adding a regularization factor is found to achieve a different result.\n","\n","$$ \\alpha_* = argmin_{\\alpha} L(\\alpha,x_0) + R(\\alpha) $$\n","\n","If carefully selected dependend on the task, the regularization term combined with the right choice of function can be used for the image restoration task, to achieve an estimate of $x$, the clean image. An example given for R(x) is total variation (TV) which leads the optimization path away from noisy outcomes. The regularization term, crafted for the particular task can also be seen to capture the prior information despite it being generic. The total variation regularizer capturing the smooth surface features for the denoising task is an example of this generic prior extraction. \n","\n","### The Deep Image Prior Method\n","\n","Combining the frameworks introduced above, the researchers introduced the following minimizer for the task of generating the clean image.\n","\n","$$ \\theta_* = argmin_{\\theta} L(f_{\\theta}(z),x_0)$$\n","\n","where $z$ is the random image input. Compared with the approach denoted in the previous subsection, the optimization is done over the network parameters denoted with $\\theta$ instead of the input variables that are expected to be transformed into the clean image. The corrupted image is still used as the target of the loss minimization but the regularizer which was able to capture generic prior information is now replaced by the deep network which provides a powerful parameters for capturing the prior information specific to the image that is being processed. \n","\n","Contrary to the approach discussed in the first section, this approach involves optimization, not training with the aim of optimized parameters $\\theta$ acquiring the prior information needed to produce the clean image as such;\n","\n","$$ x = f_{\\theta_{i}(z)}$$\n","\n","where $\\theta_{**}$ denotes parameters during iteration i, given an iterative optimization is employed. The random image $z$ remains the same throughout the optimization. \n","\n","Note that this \"useful\" parameters are not those that are solutions of the optimization problem denoted as $\\theta_*$ which leads to the corrupted image again. This is defined as the overfitting case of this method by the researchers, and a key aspect of this method is early stopping. \n"," \n","---\n","\n","## 3.Objective and Methods\n","\n","In this project, the inpainting task will be focused on, in which the corrupted image has lost pixels, and the task is to generate the complete image. We will employ the following loss function which is used by the researchers;\n","\n","$$ L(\\alpha,x_0) = | (\\alpha-x_0) \\circ m |^2 $$\n","\n","where $\\circ$ refers to Hadamard product which provides pixel-wise multiplicion with the mask $m$ and the resulting difference image between the current output of the network the corrupted image, which is formed by applying the mask $m$ to the clean image $x$.\n","\n","There are two types of mask that will be used in experiments, which are %50 bernoulli and text masks. The %bernoulli masked image is formed by masking %50 of pixels selected according to random distribution. \n","\n","(Bernoulli code can be put here)\n","(example of masked image\n","\n","The text mask is formed by masking the image with letters written on the image.\n","\n","(Code)\n","(example image)\n","\n","The aim of this project can be summarized to provide evaluation for claims made by the authors on how this method works while also attempting to replicate a part of the results reported. These claims are;\n","\n","*   Erroreuous early-stopping leads to undesired results\n","*   The method gives better results compared to legacy methods employed in (citations)\n","*   The network structure should resonate with the task at hand\n","*   The network is handcrafted for the particular image as a parametrization of the prior\n","*   The network should not converge quickly, and deeper networks lead to be\n","*   Adding skip connections that cause network to work \"too well\" is detrimental to the performance.\n","\n","In the first experiment \n","In the second experiment \n","In the third experiment \n","---\n","\n","## 4.Default Architecture\n","\n","The architecture that is employed in our experiments is based on the model specification provided by the researchers, and the existing pytorch module for generating the network is utilized. The figure below illustrates the components of the default architecture;\n","\n","(figure for architecture)\n","\n","\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cq9cm5m0ZVY4","colab_type":"text"},"source":["## 5.Experiment 1: Replication of Table for Bernoulli Mask\n","### Goal\n","### What Did We Change?\n","### Experimental Setup\n","### Results\n","\n","![PSNRs of each image](https://drive.google.com/file/d/1Y5o0Ll3xqcEHBGMC2mBe3hPVeNBU-sKq/view?usp=sharing)\n","\n","![Evolution of PSNRS for each image](https://drive.google.com/file/d/1ThMhdFcE9_Y0vOPvSsaOWVuipqk8CKTt/view?usp=sharing)\n","\n","+-------------------+--------------------+-------------------+--------------------+-------------------+-------------------+------------------+--------------------+-------------------+--------------------+-------------------+-------------------+--------------------+\n","|      boat.png     |   cameraman.png    |      lena.png     |    montage.png     |    barbara.png    |   Lenacolor.png   |    house.png     |  fingerprint.png   |      man.png      |  pepperscolor.png  |      hill.png     |   peppers256.png  |     couple.png     |\n","+-------------------+--------------------+-------------------+--------------------+-------------------+-------------------+------------------+--------------------+-------------------+--------------------+-------------------+-------------------+--------------------+\n","| 32.67411860059282 | 30.949618392029716 | 35.58839253698879 | 35.498850846723975 | 33.27793767929548 | 35.82579780034698 | 44.3010065084932 | 32.223511583204726 | 33.46124336069567 | 33.069062491507026 | 33.14717939932711 | 41.63863826178637 | 32.282398932957676 |\n","+-------------------+--------------------+-------------------+--------------------+-------------------+-------------------+------------------+--------------------+-------------------+--------------------+-------------------+-------------------+--------------------+\n","\n","\n","---\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cw_OTluFZZdC","colab_type":"text"},"source":["## 6.Experiment 2: Architecture Modification\n","### Goal\n","### What Did We Change?\n","### Experimental Setup\n","### Results\n","\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XnuBcMYnZblL","colab_type":"text"},"source":["## 7.Experiment 3: Hyperparameter Testing\n","### Goal\n","### What Did We Change?\n","### Experimental Setup\n","### Results\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5PV6TjCaZu1k","colab_type":"text"},"source":["##8.Discussion\n","\n","\n","---\n","\n","\n","##9.Conclusion\n","\n","\n","---\n","\n"]}]}